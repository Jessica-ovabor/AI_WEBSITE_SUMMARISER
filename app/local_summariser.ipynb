{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c16a657",
   "metadata": {},
   "source": [
    "This notebook calls ollama locally without making request to the live API. This uses local model that run directly on my computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df617d",
   "metadata": {},
   "source": [
    "Install Ollama\n",
    "\n",
    "Ollama pull any_model\n",
    "\n",
    "Ollama serve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37396611",
   "metadata": {},
   "source": [
    "Import dependencies and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from scraper import web_scraper\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = \"Any_Key\"\n",
    "website_url = os.getenv(\"WEBSITE_URL\")\n",
    "\n",
    "\n",
    "if not api_key:\n",
    "    print(\"Error: API_KEY is required\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b4155",
   "metadata": {},
   "source": [
    "Check if Ollama is running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448095e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_request = requests.get(\"http://localhost:11434/\")\n",
    "\n",
    "ollama_request.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705a0e5",
   "metadata": {},
   "source": [
    "Define OpenAI client library for Locally sourced model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c174b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38acb9c",
   "metadata": {},
   "source": [
    "System Prompts and User Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8791a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an skillful AI assitant who is skilled in summarising any content. You provide short and concise summarisation while keeping all the main point intact. The content you provide is meaningful and well structured\"\n",
    "\n",
    "user_prompt = web_scraper(website_url)\n",
    "\n",
    "print(user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = ollama_client.chat.completions.create(\n",
    "model=\"qwen2.5:1.5b\",\n",
    "messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "    \n",
    "\n",
    ")\n",
    "print(ollama_model.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-website-summariser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
